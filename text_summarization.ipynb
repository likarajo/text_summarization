{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Wikipedia Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import ssl\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/Natural_language_processing'\n",
    "\n",
    "gcontext = ssl.SSLContext()\n",
    "scraped_data = urllib.request.urlopen(url, context=gcontext)\n",
    "article = scraped_data.read()\n",
    "parsed_article = bs.BeautifulSoup(article, 'lxml')\n",
    "paragraphs = parsed_article.find_all('p')\n",
    "\n",
    "text = \"\"\n",
    "for p in paragraphs:\n",
    "    text += p.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess and format the data\n",
    "* Remove square bracketted links\n",
    "* Remove special characters and digits\n",
    "* Remove extra spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text_pp = re.sub(r'\\[[0-9]*\\]', ' ', text)\n",
    "text_pp_formatted = re.sub(r'[^a-zA-Z]', ' ', text_pp)\n",
    "text_pp_formatted = re.sub(r'\\s+', ' ', text_pp_formatted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the text to sentences and to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "sentences = nltk.sent_tokenize(text_pp)\n",
    "words = nltk.word_tokenize(text_pp_formatted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the Word Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "word_frequencies = {}\n",
    "for word in words:\n",
    "    if word not in stop_words:\n",
    "        if word not in word_frequencies.keys():\n",
    "            word_frequencies[word] = 1\n",
    "        else:\n",
    "            word_frequencies[word] += 1     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the Weighted Word Frequencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_frequency = max(word_frequencies.values())\n",
    "\n",
    "weighted_word_frequencies = {}\n",
    "for word in word_frequencies.keys():\n",
    "    weighted_word_frequencies[word] = word_frequencies[word]/max_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the Sentence Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_scores = {}\n",
    "\n",
    "for sentence in sentences:\n",
    "    words_in_sentence = nltk.word_tokenize(sentence.lower())\n",
    "    for word in words_in_sentence:\n",
    "        if word in word_frequencies.keys():\n",
    "            if len(sentence.split(' ')) < 30:\n",
    "                if sentence not in sentence_scores.keys():\n",
    "                    sentence_scores[sentence] = word_frequencies[word]\n",
    "                else:\n",
    "                    sentence_scores[sentence] += word_frequencies[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort the sentences on descending sentence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation. Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing. Systems based on machine-learning algorithms have many advantages over hand-produced rules:\n",
      "The following is a list of some of the most commonly researched tasks in natural language processing. Since the so-called \"statistical revolution\"   in the late 1980s and mid-1990s, much natural language processing research has relied heavily on machine learning. Little further research in machine translation was conducted until the late 1980s when the first statistical machine translation systems were developed. Up to the 1980s, most natural language processing systems were based on complex sets of hand-written rules. Some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing hand-written rules.\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "\n",
    "top_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)\n",
    "summary = ' '.join(top_sentences)\n",
    "\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
